{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01a6c58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7644cee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X columns: ['Amount_sum', 'Amount_mean', 'Amount_std', 'Amount_max', 'Amount_min', 'Amount_count', 'Value_sum', 'Value_mean', 'Value_std', 'Value_max', 'Value_min', 'TransactionHour_nunique', 'TransactionDay_nunique', 'TransactionMonth_nunique']\n",
      "Transformed X shape: (3742, 14)\n",
      "Feature extraction complete. Processed data ready for modeling.\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"../data/processed/eda_data.csv\")\n",
    "\n",
    "# Feature Engineering\n",
    "agg_df = df.groupby('CustomerId').agg({\n",
    "    'Amount': ['sum', 'mean', 'std', 'max', 'min', 'count'],\n",
    "    'Value': ['sum', 'mean', 'std', 'max', 'min'],\n",
    "    'TransactionHour': 'nunique',\n",
    "    'TransactionDay': 'nunique',\n",
    "    'TransactionMonth': 'nunique'\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten column names\n",
    "agg_df.columns = ['CustomerId'] + ['_'.join(col).strip() for col in agg_df.columns[1:]]\n",
    "customer_ids = agg_df['CustomerId']\n",
    "\n",
    "num_features = [col for col in agg_df.columns if agg_df[col].dtype in ['int64', 'float64'] and col != 'CustomerId']\n",
    "\n",
    "\n",
    "X = agg_df.drop(columns=['CustomerId'])\n",
    "y = np.zeros(X.shape[0]) \n",
    "\n",
    "print(\"X columns:\", X.columns.tolist())\n",
    "\n",
    "numeric_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Apply transformations\n",
    "X_scaled = numeric_pipeline.fit_transform(X)\n",
    "\n",
    "print(\"Transformed X shape:\", X_scaled.shape)\n",
    "\n",
    "print(\"Feature extraction complete. Processed data ready for modeling.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13d6ae5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proxy labels created. High-risk cluster: 0\n",
      "is_high_risk\n",
      "0    2307\n",
      "1    1435\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Task 4 : proxy labels creation\n",
    "from sklearn.cluster import KMeans\n",
    "import seaborn as sns\n",
    "\n",
    "df['TransactionStartTime'] = pd.to_datetime(df['TransactionStartTime'], errors='coerce')\n",
    "\n",
    "# Define snapshot date for Recency calculation\n",
    "snapshot_date = df['TransactionStartTime'].max() + pd.Timedelta(days=1)\n",
    "\n",
    "# Calculate RFM per CustomerId\n",
    "rfm = df.groupby('CustomerId').agg({\n",
    "    'TransactionStartTime': lambda x: (snapshot_date - x.max()).days,\n",
    "    'TransactionId': 'count',\n",
    "    'Value': 'sum'\n",
    "}).reset_index()\n",
    "rfm.columns = ['CustomerId', 'Recency', 'Frequency', 'Monetary']\n",
    "\n",
    "# Scale RFM for clustering\n",
    "scaler = StandardScaler()\n",
    "rfm_scaled = scaler.fit_transform(rfm[['Recency', 'Frequency', 'Monetary']])\n",
    "\n",
    "# KMeans Clustering (3 segments)\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "rfm['Cluster'] = kmeans.fit_predict(rfm_scaled)\n",
    "\n",
    "# Determine high-risk cluster: lowest Frequency + Monetary, highest Recency\n",
    "cluster_summary = rfm.groupby('Cluster').agg({\n",
    "    'Recency': 'mean',\n",
    "    'Frequency': 'mean',\n",
    "    'Monetary': 'mean'\n",
    "}).sort_values(by='Frequency')\n",
    "\n",
    "high_risk_cluster = cluster_summary.index[0]  # assume lowest freq is highest risk\n",
    "rfm['is_high_risk'] = (rfm['Cluster'] == high_risk_cluster).astype(int)\n",
    "\n",
    "# Save or merge this label with processed features\n",
    "# e.g. rfm[['CustomerId', 'is_high_risk']] \n",
    "#merge it with df\n",
    "\n",
    "rfm = rfm[['CustomerId', 'is_high_risk']]\n",
    "df = df.merge(rfm, on='CustomerId', how='left')\n",
    "\n",
    "# Save the processed data with proxy labels\n",
    "PROCESSED_DATA_PATH = \"../data/processed/eda_data_with_proxy_labels.csv\"\n",
    "df.to_csv(PROCESSED_DATA_PATH, index=False)\n",
    "\n",
    "print(\"Proxy labels created. High-risk cluster:\", high_risk_cluster)\n",
    "print(rfm['is_high_risk'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
